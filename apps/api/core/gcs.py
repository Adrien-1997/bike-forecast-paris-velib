"""GCS helpers for the API layer.

This module centralizes **low-level access to Google Cloud Storage** for the API:

- minimal helpers to:
  - parse `gs://bucket/key` URIs,
  - instantiate a `storage.Client`,
  - compute standard HTTP-ish headers (ETag, Last-Modified, Cache-Control),
- direct read helpers:
  - `read_blob_bytes`, `read_blob_text`, `read_blob_json`, `head_blob`,
- an in-memory cache with TTL + ETag validation:
  - `read_blob_bytes_cached`, `read_blob_text_cached`, `read_blob_json_cached`.

Typical usage in the API:

- read JSON artefacts generated by batch jobs
  (monitoring, serving forecasts, etc.),
- reuse ETag + Last-Modified to build HTTP responses (conditional gets),
- avoid hammering GCS on frequently requested endpoints thanks to the cache.
"""

from __future__ import annotations

import json
import time
from typing import Tuple, Dict, Any, Optional
from datetime import datetime, timezone
from email.utils import format_datetime

try:
    from google.cloud import storage  # type: ignore
except Exception:
    # Le module est optionnel à l'import; on lève une erreur explicite
    # au moment de son utilisation si absent.
    storage = None  # vérifié à l'usage


# ───────────────────────── Low-level helpers ─────────────────────────
def _require_client() -> "storage.Client":
    """Return a configured `google.cloud.storage.Client` or raise if unavailable.

    Raises
    ------
    RuntimeError
        If `google-cloud-storage` is not installed / importable.
    """
    if storage is None:
        raise RuntimeError("google-cloud-storage is not installed")
    return storage.Client()


def _parse_gs(uri: str) -> Tuple[str, str]:
    """Split a GCS URI `gs://bucket/key` into `(bucket, key)`.

    Parameters
    ----------
    uri : str
        GCS URI starting with `gs://`.

    Returns
    -------
    tuple[str, str]
        `(bucket, key)` components.

    Raises
    ------
    AssertionError
        If the URI does not start with `gs://`.
    """
    assert uri.startswith("gs://"), f"invalid GCS uri: {uri}"
    bucket, key = uri[5:].split("/", 1)
    return bucket, key


def _std_headers(blob) -> Dict[str, str]:
    """Build a standard set of HTTP-like headers from a GCS blob.

    Headers include:

    - `Cache-Control` : always `"public, max-age=60"` (short-lived cache),
    - `ETag`          : if available on the blob,
    - `Last-Modified` : RFC-2822 date in UTC if `blob.updated` is a datetime.

    Parameters
    ----------
    blob :
        GCS blob object as returned by the storage client.

    Returns
    -------
    dict[str, str]
        Headers dictionary suitable to attach to an HTTP response.
    """
    etag = getattr(blob, "etag", None)
    updated = getattr(blob, "updated", None)
    if isinstance(updated, datetime):
        last_mod = format_datetime(updated.astimezone(timezone.utc))
    else:
        last_mod = None

    headers: Dict[str, str] = {"Cache-Control": "public, max-age=60"}
    if etag:
        headers["ETag"] = etag
    if last_mod:
        headers["Last-Modified"] = last_mod
    return headers


# ───────────────────────── Public: direct read ─────────────────────────
def read_blob_bytes(gs_uri: str) -> Tuple[bytes, Dict[str, str]]:
    """Download a blob from GCS and return its raw bytes + headers.

    Parameters
    ----------
    gs_uri : str
        GCS URI of the object to read.

    Returns
    -------
    tuple[bytes, dict[str, str]]
        `(data_bytes, headers)` where headers is built by `_std_headers`.
    """
    client = _require_client()
    bkt, key = _parse_gs(gs_uri)
    blob = client.bucket(bkt).blob(key)
    data = blob.download_as_bytes()
    headers = _std_headers(blob)
    return data, headers


def read_blob_text(gs_uri: str, encoding: str = "utf-8") -> Tuple[str, Dict[str, str]]:
    """Download a GCS blob and decode it as text.

    Parameters
    ----------
    gs_uri : str
        GCS URI of the object.
    encoding : str, default "utf-8"
        Text encoding used for decoding the bytes.

    Returns
    -------
    tuple[str, dict[str, str]]
        `(decoded_text, headers)`.
    """
    raw, headers = read_blob_bytes(gs_uri)
    return raw.decode(encoding), headers


def read_blob_json(gs_uri: str) -> Tuple[Any, Dict[str, str]]:
    """Download a GCS blob and parse it as JSON.

    Parameters
    ----------
    gs_uri : str
        GCS URI of the JSON document.

    Returns
    -------
    tuple[Any, dict[str, str]]
        `(parsed_json, headers)`.
    """
    txt, headers = read_blob_text(gs_uri)
    return json.loads(txt), headers


def head_blob(gs_uri: str) -> Dict[str, str]:
    """Fetch metadata of a GCS blob and return headers built from it.

    This issues a `get_blob` call and does **not** download the content.

    Parameters
    ----------
    gs_uri : str
        GCS URI of the object.

    Returns
    -------
    dict[str, str]
        Headers dictionary produced by `_std_headers`.

    Raises
    ------
    FileNotFoundError
        If the blob does not exist.
    """
    client = _require_client()
    bkt, key = _parse_gs(gs_uri)
    blob = client.bucket(bkt).get_blob(key)
    if blob is None:
        raise FileNotFoundError(gs_uri)
    return _std_headers(blob)


# ───────────────────────── In-memory cache (TTL + ETag) ─────────────────────────
# cache: uri -> { data: bytes, headers: {...}, etag: str|None, expire: float }
_CACHE: Dict[str, Dict[str, Any]] = {}


def _now() -> float:
    """Return current epoch time in seconds (float)."""
    return time.time()


def _cache_get(uri: str) -> Optional[Dict[str, Any]]:
    """Get a cached entry for a URI if not expired.

    Parameters
    ----------
    uri : str
        GCS URI used as cache key.

    Returns
    -------
    dict[str, Any] | None
        Cache entry with `data`, `headers`, `etag`, `expire` if valid,
        otherwise None (not present or expired).
    """
    item = _CACHE.get(uri)
    if not item:
        return None
    if item["expire"] < _now():
        # expired -> remove and return None
        _CACHE.pop(uri, None)
        return None
    return item


def _cache_set(uri: str, data: bytes, headers: Dict[str, str], ttl: int) -> None:
    """Store a bytes payload + headers in the in-memory cache.

    Parameters
    ----------
    uri : str
        GCS URI used as cache key.
    data : bytes
        Raw bytes of the blob.
    headers : dict[str, str]
        Headers associated with the blob (used for ETag + Last-Modified).
    ttl : int
        Time-to-live in seconds.
    """
    _CACHE[uri] = {
        "data": data,
        "headers": headers,
        "etag": headers.get("ETag"),
        "expire": _now() + max(1, int(ttl)),
    }


def read_blob_bytes_cached(gs_uri: str, ttl_seconds: int = 60) -> Tuple[bytes, Dict[str, str]]:
    """
    Lit un blob avec cache mémoire (TTL) + validation ETag.

    Stratégie :
    - Si une entrée de cache non expirée existe → renvoie directement.
    - Si l'entrée est expirée :
        * on fait un HEAD pour récupérer le nouvel ETag,
        * si l'ETag est identique à celui du cache → on prolonge le TTL
          et on réutilise les bytes en mémoire,
        * sinon on retélécharge le blob et on met à jour le cache.

    Parameters
    ----------
    gs_uri : str
        GCS URI of the object to read.
    ttl_seconds : int, default 60
        Time-to-live for the cached entry (seconds).

    Returns
    -------
    tuple[bytes, dict[str, str]]
        `(data_bytes, headers)`.

    Raises
    ------
    FileNotFoundError
        If the blob does not exist.
    """
    # 1) cache non expiré
    cached = _cache_get(gs_uri)
    if cached is not None:
        return cached["data"], cached["headers"]

    # 2) cache expiré → HEAD pour comparer ETag (si on en a un)
    client = _require_client()
    bkt, key = _parse_gs(gs_uri)
    blob = client.bucket(bkt).get_blob(key)
    if blob is None:
        raise FileNotFoundError(gs_uri)
    headers_head = _std_headers(blob)
    etag_head = headers_head.get("ETag")

    prev = _CACHE.get(gs_uri)
    if prev and prev.get("etag") and etag_head and prev["etag"] == etag_head:
        # Même ETag → on peut réutiliser les bytes et juste prolonger le TTL
        _cache_set(gs_uri, prev["data"], headers_head, ttl_seconds)
        return prev["data"], headers_head

    # 3) télécharger frais
    data = blob.download_as_bytes()
    headers = _std_headers(blob)
    _cache_set(gs_uri, data, headers, ttl_seconds)
    return data, headers


def read_blob_text_cached(gs_uri: str, ttl_seconds: int = 60, encoding: str = "utf-8") -> Tuple[str, Dict[str, str]]:
    """Version textuelle de `read_blob_bytes_cached`.

    Parameters
    ----------
    gs_uri : str
        GCS URI of the object to read.
    ttl_seconds : int, default 60
        Cache TTL in seconds.
    encoding : str, default "utf-8"
        Encoding used to decode the bytes.

    Returns
    -------
    tuple[str, dict[str, str]]
        `(decoded_text, headers)`.
    """
    raw, headers = read_blob_bytes_cached(gs_uri, ttl_seconds=ttl_seconds)
    return raw.decode(encoding), headers


def read_blob_json_cached(gs_uri: str, ttl_seconds: int = 60) -> Tuple[Any, Dict[str, str]]:
    """Version JSON de `read_blob_bytes_cached`.

    Parameters
    ----------
    gs_uri : str
        GCS URI of the object to read.
    ttl_seconds : int, default 60
        Cache TTL in seconds.

    Returns
    -------
    tuple[Any, dict[str, str]]
        `(parsed_json, headers)`.
    """
    txt, headers = read_blob_text_cached(gs_uri, ttl_seconds=ttl_seconds)
    return json.loads(txt), headers
