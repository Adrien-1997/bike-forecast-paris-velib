name: velib-ingest

on:
  schedule:
    - cron: "*/20 * * * *"   # un démarrage toutes les 20 min
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: velib-ingest
  cancel-in-progress: true

jobs:
  looped_ingest:
    runs-on: ubuntu-latest
    timeout-minutes: 40
    env:
      TZ: Europe/Paris
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      PIP_NO_INPUT: "1"

    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements-pipeline.txt --prefer-binary --no-build-isolation
          pip install -U huggingface_hub

      - name: Run 4 cycles (every ~5 min) with boundary alignment
        shell: bash
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET: ${{ vars.HF_DATASET }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          set -euo pipefail

          align_5min() {
            # Aligne sur le prochain multiple de 300s (frontière 5 min) en UTC
            now=$(date -u +%s)
            wait=$(( ( (now/300 + 1)*300 ) - now ))
            printf "[align] sleeping %ss to next 5-min boundary\n" "$wait"
            sleep "$wait"
          }

          do_cycle() {
            echo "----- cycle start: $(date -u '+UTC %F %T') -----"
            # petit tampon pour laisser OpenData publier
            sleep 45

            python -m src.ingest
            python -m src.aggregate

            # build snapshot (dernier bin)
            python - <<'PY'
            import pathlib, pandas as pd, datetime as dt
            DOCS = pathlib.Path('docs'); p = DOCS/'exports'/'velib.parquet'
            if not p.exists(): raise SystemExit("no velib.parquet")
            df = pd.read_parquet(p)
            ts_col = next((c for c in ('tbin_utc','timestamp','ts','time') if c in df.columns), None)
            if ts_col is None: raise SystemExit("no ts column")
            last_ts = df[ts_col].max()
            snap = df[df[ts_col]==last_ts].copy()
            if pd.api.types.is_datetime64_any_dtype(snap[ts_col]):
                ts_utc = pd.to_datetime(last_ts, utc=True)
                ts_str = ts_utc.strftime('%Y%m%d_%H%M')
            else:
                ts_str = dt.datetime.now(dt.timezone.utc).strftime('%Y%m%d_%H%M')
            out_dir = DOCS/'exports'/'snapshots'; out_dir.mkdir(parents=True, exist_ok=True)
            out = out_dir/f'velib_{ts_str}.parquet'
            snap.to_parquet(out, index=False)
            print('[snapshot] built', out)
            PY

            # upload snapshot to HF (API)
            python - <<'PY'
            import os, pathlib, re
            from huggingface_hub import HfApi
            repo_id = os.environ['HF_DATASET']; token = os.environ['HF_TOKEN']
            api = HfApi(token=token)
            snaps = sorted(pathlib.Path('docs/exports/snapshots').glob('velib_*.parquet'))
            if not snaps: raise SystemExit("no snapshot")
            path = snaps[-1]
            m = re.match(r'velib_(\d{8})_(\d{4})\.parquet$', path.name)
            if m:
                y, mo, d = m.group(1)[:4], m.group(1)[4:6], m.group(1)[6:8]
                dest = f'snapshots/{y}/{mo}/{d}/{path.name}'
            else:
                dest = f'snapshots/{path.name}'
            api.upload_file(path_or_fileobj=str(path), path_in_repo=dest,
                            repo_id=repo_id, repo_type="dataset",
                            commit_message=f"Add snapshot {path.name}")
            print("[upload] snapshot ->", dest)
            PY

            echo "----- cycle end: $(date -u '+UTC %F %T') -----"
          }

          # aligne au début
          align_5min
          # 4 cycles (≈ 20 min)
          for i in 1 2 3 4; do
            do_cycle
            # aligne avant le cycle suivant
            [ "$i" -lt 4 ] && align_5min
          done
