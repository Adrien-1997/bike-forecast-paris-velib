name: velib-ingest

on:
  schedule:
    # Toutes les 5 minutes (UTC)
    - cron: "*/5 * * * *"
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: velib-ingest
  cancel-in-progress: true

jobs:
  ingest_aggregate_and_push_snapshot:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      TZ: Europe/Paris
      MPLBACKEND: Agg
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      PIP_NO_INPUT: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Heartbeat (log trigger times)
        shell: bash
        run: |
          set -euo pipefail
          echo "event=${{ github.event_name }}"
          echo "ref=${{ github.ref }}"
          echo "sha=${{ github.sha }}"
          date -u "+UTC   %Y-%m-%d %H:%M:%S"
          date    "+PARIS %Y-%m-%d %H:%M:%S %Z"

      - name: Install pipeline deps (prefer wheels)
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install -U pip
          pip install -r requirements-pipeline.txt --prefer-binary --no-build-isolation

      # Tampon pour laisser l'OpenData publier
      - name: Offset start (~60s)
        shell: bash
        run: sleep 60

      - name: Ingest snapshot (OpenData → DuckDB)
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: python -m src.ingest

      # ✅ Aggregate à chaque tick → fraîcheur 5 min pour l'app
      - name: Aggregate 5min + weather → docs/exports/
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: python -m src.aggregate

      - name: Inspect exports (quick sanity checks)
        shell: bash
        run: |
          set -euo pipefail
          echo "=== Listing docs/exports ==="
          ls -lh docs/exports || true
          python - <<'PY'
          import pathlib, pandas as pd
          p = pathlib.Path('docs/exports/velib.parquet')
          print('[ingest] velib.parquet exists:', p.exists())
          if p.exists():
              df = pd.read_parquet(p)
              print('[ingest] rows:', len(df), 'unique stations:', df['stationcode'].nunique() if 'stationcode' in df else 'n/a')
              # Affiche le dernier bin disponible
              ts_col = 'tbin_utc' if 'tbin_utc' in df.columns else ('timestamp' if 'timestamp' in df.columns else None)
              if ts_col:
                  print('[ingest] last bin:', df[ts_col].max())
          PY

      # -------- Build & push a compact 5-min snapshot to HF (API upload) --------
      - name: Install Hugging Face client
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install -U huggingface_hub

      - name: Build last-5min snapshot (single bin) from aggregate
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, pathlib, pandas as pd
          import datetime as dt
          DOCS = pathlib.Path('docs')
          exports = DOCS / 'exports' / 'velib.parquet'
          if not exports.exists():
              raise SystemExit("velib.parquet not found; abort snapshot build")

          df = pd.read_parquet(exports)

          # Détecte la colonne temporelle
          ts_col = None
          for c in ('tbin_utc', 'timestamp', 'ts', 'time'):
              if c in df.columns:
                  ts_col = c
                  break
          if ts_col is None:
              raise SystemExit("No timestamp column found in velib.parquet")

          # Dernier bin complet
          last_ts = df[ts_col].max()
          snap = df[df[ts_col] == last_ts].copy()

          # Nom: velib_YYYYMMDD_HHMM.parquet (UTC si possible)
          if pd.api.types.is_datetime64_any_dtype(snap[ts_col]):
              ts_utc = pd.to_datetime(last_ts, utc=True)
              ts_str = ts_utc.strftime('%Y%m%d_%H%M')
          else:
              ts_str = dt.datetime.now(dt.timezone.utc).strftime('%Y%m%d_%H%M')

          out_dir = DOCS / 'exports' / 'snapshots'
          out_dir.mkdir(parents=True, exist_ok=True)
          out_file = out_dir / f'velib_{ts_str}.parquet'
          snap.to_parquet(out_file, index=False)
          print('[snapshot] built:', out_file, 'rows:', len(snap))
          PY

      - name: Upload snapshot to Hugging Face (no git clone)
        shell: bash
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_DATASET: ${{ vars.HF_DATASET }}   # ex: "adrien/velib-paris-exports"
        run: |
          set -euo pipefail
          if [[ -z "${HF_TOKEN:-}" || -z "${HF_DATASET:-}" ]]; then
            echo "HF_TOKEN (secret) or HF_DATASET (variable) missing."
            exit 1
          fi
          python - <<'PY'
          import os, pathlib, re
          from huggingface_hub import HfApi
          repo_id = os.environ['HF_DATASET']
          token = os.environ['HF_TOKEN']
          api = HfApi(token=token)

          # Snapshot le plus récent
          snaps_dir = pathlib.Path('docs/exports/snapshots')
          snaps = sorted(snaps_dir.glob('velib_*.parquet'))
          if not snaps:
              raise SystemExit("No snapshot parquet to upload")
          path = snaps[-1]

          # Dossier daté: snapshots/YYYY/MM/DD/...
          m = re.match(r'velib_(\d{8})_(\d{4})\.parquet$', path.name)
          if m:
              y, mo, d = m.group(1)[:4], m.group(1)[4:6], m.group(1)[6:8]
              dest = f'snapshots/{y}/{mo}/{d}/{path.name}'
          else:
              dest = f'snapshots/{path.name}'

          api.upload_file(
              path_or_fileobj=str(path),
              path_in_repo=dest,
              repo_id=repo_id,
              repo_type="dataset",
              commit_message=f"Add snapshot {path.name}"
          )
          print("[upload] uploaded to HF:", repo_id, dest)
          PY
